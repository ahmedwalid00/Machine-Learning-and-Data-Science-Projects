{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pBQBTF-If3f5",
        "outputId": "0d12490c-c52f-4036-83b0-424f24c5527f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\User\\\\Downloads'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd743ac09442>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\User\\Downloads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\User\\\\Downloads'"
          ]
        }
      ],
      "source": [
        "#Feature engineering and Modeling with Evaluation to Black friday\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "os.chdir(r\"C:\\Users\\User\\Downloads\")\n",
        "\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "df = pd.concat([df_train,df_test],ignore_index=True)\n",
        "df.head()\n",
        "df.info()\n",
        "\n",
        "#different Feature engineering techniques\n",
        "#===========================================\n",
        "\n",
        "#droping non important columns\n",
        "df.drop(['User_ID' , 'Product_ID'] , axis = 1 , inplace =True)\n",
        "df.head()\n",
        "\n",
        "#Dealing with Gender feature\n",
        "df['Gender'] = pd.get_dummies(df['Gender'] , drop_first=True)\n",
        "df['Gender']\n",
        "df.shape\n",
        "\n",
        "#Dealing with Age feature\n",
        "df['Age'].unique()\n",
        "df['Age'] = df['Age'].map({'0-17' : 1 , '18-25' : 2 , '26-35' : 3 , '36-45' : 4 , '46-50' : 5 , '51-55' : 6 ,'55+' : 7})\n",
        "df['Age'].tail(5)\n",
        "\n",
        "\n",
        "\n",
        "#Dealing with City Category feature\n",
        "df.info()\n",
        "df['City_Category'].head(5)\n",
        "df['City_Category'].unique()\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first' , sparse_output=False)\n",
        "df['City_Category'] = encoder.fit_transform(df[['City_Category']])\n",
        "df['City_Category'].tail(5)\n",
        "df.shape\n",
        "df.info()\n",
        "\n",
        "#Dealing with null values of Product Category 2\n",
        "df.isnull().sum()\n",
        "df['Product_Category_2'].unique()\n",
        "df['Product_Category_2'].value_counts()\n",
        "df['Product_Category_2'] = df['Product_Category_2'].fillna(df['Product_Category_2'].mode()[0])\n",
        "df['Product_Category_2'].isnull().sum()\n",
        "\n",
        "##Dealing with null values of Product Category 3\n",
        "df.isnull().sum()\n",
        "df['Product_Category_3'].unique()\n",
        "df['Product_Category_3'] = df['Product_Category_3'].fillna(df['Product_Category_3'].mode()[0])\n",
        "df['Product_Category_3'].isnull().sum()\n",
        "\n",
        "#Dealing with Stay In Current City Years feature\n",
        "df.info()\n",
        "df['Stay_In_Current_City_Years'].head(5)\n",
        "df['Stay_In_Current_City_Years'].unique()\n",
        "df['Stay_In_Current_City_Years']=df['Stay_In_Current_City_Years'].str.replace('+','')\n",
        "df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].astype(int)\n",
        "\n",
        "#Dealing with Missing values in the targe column\n",
        "df['Purchase'].isnull().sum()\n",
        "df['Purchase'].unique()\n",
        "df['Purchase'] = df['Purchase'].fillna(df['Purchase'].mean())\n",
        "df['Purchase'].isnull().sum()\n",
        "\n",
        "#Feature scaling\n",
        "df.info()\n",
        "x = df.drop('Purchase' , axis= 1)\n",
        "y = df['Purchase']\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(x)\n",
        "x\n",
        "\n",
        "#Spliting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train , x_test , y_train , y_test = train_test_split(x, y , train_size=0.8,random_state=10)\n",
        "\n",
        "#data is ready for modeling now\n",
        "#==================================\n",
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error , mean_squared_error , r2_score\n",
        "\n",
        "#apply grid search to select the best model with the best parameters\n",
        "\n",
        "models = [ ('Lasso' , LassoCV() ,\n",
        "            {'max_iter': [1000, 5000, 10000],\n",
        "             'tol': [1e-4, 1e-3, 1e-2]} ) ,\n",
        "\n",
        "          ('Support Vector Machine' ,SVR() ,\n",
        "           {'C':[0.1 , 1 , 10] ,'kernel' :['rbf' , 'poly','sigmoid','linear'] ,\n",
        "            'gamma': ['scale', 'auto', 0.1, 1] } ) ,\n",
        "\n",
        "          ('Random Forest' , RandomForestRegressor() ,\n",
        "           {'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20, 30] , 'min_samples_split': [2, 5, 10]}\n",
        "          )\n",
        "    ]\n",
        "\n",
        "best_model = None\n",
        "best_parameters = None\n",
        "best_score = 0\n",
        "\n",
        "for name, model, model_param in models:\n",
        "    grid_search = GridSearchCV(model, model_param, cv=5, scoring='neg_mean_squared_error', n_jobs=-1 , error_score='raise')\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    print(f\"model : {name} ---> score : {grid_search.best_score_}\")\n",
        "    print(\"===============================\")\n",
        "\n",
        "    if best_score < grid_search.best_score_:\n",
        "        best_model = model\n",
        "        best_parameters = grid_search.best_params_\n",
        "        best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"best model : {best_model}\")\n",
        "print(f\"best parameters : {best_parameters}\")\n",
        "print(f\"best score : {best_score}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}